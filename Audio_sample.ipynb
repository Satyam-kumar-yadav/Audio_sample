{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqpY0vRlFBAi",
        "outputId": "4a1082ae-b283-494d-ca21-a9c67aee9a0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWeHCikicSrs",
        "outputId": "7323e9ca-0109-48f2-a2fb-fe460e2b34f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gmGbaF2ze8HO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = '/content/gdrive/MyDrive/audio'"
      ],
      "metadata": {
        "id": "V2s3ElQSfdry"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(p):\n",
        "    print(os.path.join(p,i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCauJnOner0E",
        "outputId": "6e74767b-cccb-47f7-b6af-a6eec352925a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/audio/wav1.wav\n",
            "/content/gdrive/MyDrive/audio/wav2.wav\n",
            "/content/gdrive/MyDrive/audio/wav3.wav\n",
            "/content/gdrive/MyDrive/audio/wav4.wav\n",
            "/content/gdrive/MyDrive/audio/wav5.wav\n",
            "/content/gdrive/MyDrive/audio/wav1_4.wav\n",
            "/content/gdrive/MyDrive/audio/wav1_2.wav\n",
            "/content/gdrive/MyDrive/audio/wav1_3.wav\n",
            "/content/gdrive/MyDrive/audio/wav1_5.wav\n",
            "/content/gdrive/MyDrive/audio/wav1_1.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "arr = []\n",
        "arr_2 = []\n",
        "for i in sorted(os.listdir(p)):\n",
        "# Load the audio file\n",
        "    print(i)\n",
        "    audio, sr = librosa.load(os.path.join(p,i), sr=16000)\n",
        "\n",
        "    # Split the audio into segments of 1 second\n",
        "    segments = librosa.effects.split(audio, top_db=30, frame_length=sr, hop_length=sr)\n",
        "\n",
        "    # Create an empty list to store the spectrograms\n",
        "    spectrograms = []\n",
        "\n",
        "    # Iterate through the segments\n",
        "    for segment in segments:\n",
        "        \n",
        "        # Apply a Hann window to the segment\n",
        "        audio_segment = audio[segment[0]:segment[1]]\n",
        "        \n",
        "        # Apply Hann window to segment\n",
        "        window = np.hanning(len(audio_segment))\n",
        "        audio_segment = audio_segment * window\n",
        "        arr_2.append(audio_segment)\n",
        "        # Compute the STFT of the segment\n",
        "        stft = librosa.core.stft(audio_segment, n_fft=2048, hop_length=512)\n",
        "        \n",
        "        # Compute the magnitude spectrogram\n",
        "        spectrogram = np.abs(stft)\n",
        "        print(spectrogram.shape)\n",
        "        # Normalize the spectrogram by subtracting the mean and dividing by the standard deviation\n",
        "        mean = np.mean(spectrogram)\n",
        "        std = np.std(spectrogram)\n",
        "        spectrogram = (spectrogram - mean) / std\n",
        "        \n",
        "        # Append the spectrogram to the list\n",
        "        spectrograms.append(spectrogram)\n",
        "\n",
        "    # Convert the list of spectrograms to a numpy array\n",
        "    spectrograms = np.array(spectrograms)\n",
        "    arr.append(spectrograms)\n",
        "    # Use the spectrograms as input for fine-tuning the Wav2Vec2-base model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWAIzEdlEaz",
        "outputId": "3d4f2a04-6fe2-4473-c415-ba277e167d59"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav1.wav\n",
            "(1025, 160)\n",
            "wav1_1.wav\n",
            "(1025, 157)\n",
            "wav1_2.wav\n",
            "(1025, 109)\n",
            "wav1_3.wav\n",
            "(1025, 177)\n",
            "wav1_4.wav\n",
            "(1025, 189)\n",
            "wav1_5.wav\n",
            "(1025, 139)\n",
            "wav2.wav\n",
            "(1025, 185)\n",
            "wav3.wav\n",
            "(1025, 248)\n",
            "wav4.wav\n",
            "(1025, 319)\n",
            "wav5.wav\n",
            "(1025, 271)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(arr_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Q8iU5rZLO2",
        "outputId": "475f71da-a41e-44f7-e991-0d9001bbbc3b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model = AutoModel.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYA477_9GjG_",
        "outputId": "107d723e-c40e-4e63-c968-fb55905a0280"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'project_hid.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxvfx4jrEisz",
        "outputId": "d5632c9f-a9bb-4716-c452-a80afc33ec03"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1025, 160)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "spectrogram_1 = torch.from_numpy(arr_2[1])\n",
        "# spectrogram_1 = spectrogram_1.squeeze(0)\n",
        "# spectrogram_1 = spectrogram_1.float()\n",
        "expected_size = (1025, 512)\n",
        "current_size = spectrogram_1.shape\n",
        "# padding = (expected_size[0] - current_size[0], expected_size[1] - current_size[1])\n",
        "\n",
        "# Pad the spectrogram\n",
        "# spectrogram_padded = F.pad(spectrogram_1, padding, mode='constant', value=0)\n",
        "\n",
        "# Pass the padded spectrogram through the model\n",
        "output = model(spectrogram_1)\n"
      ],
      "metadata": {
        "id": "NrJtzEil5ZyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['last_hidden_state'].squeeze(1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOACnQgZLVoe",
        "outputId": "a67ec207-04b1-4589-abc7-bbfca174e891"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1025, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# create a list to store the outputs\n",
        "output_list = []\n",
        "\n",
        "# loop through the dataset and get the output from the model\n",
        "for i in range(1):\n",
        "    spectrogram_1 = torch.from_numpy(arr[i])\n",
        "    spectrogram_1 = spectrogram_1.squeeze(0)\n",
        "    spectrogram_1 = spectrogram_1.float()\n",
        "    expected_size = (1025, 512)\n",
        "    current_size = spectrogram_1.shape\n",
        "    padding = (expected_size[0] - current_size[0], expected_size[1] - current_size[1])\n",
        "\n",
        "    # Pad the spectrogram\n",
        "    spectrogram_padded = F.pad(spectrogram_1, padding, mode='constant', value=0)\n",
        "\n",
        "    # Pass the padded spectrogram through the model\n",
        "    output = model(spectrogram_padded)\n",
        "    output_list.append(output['last_hidden_state'].squeeze(1))\n",
        "\n",
        "# convert the output list to a dataframe\n"
      ],
      "metadata": {
        "id": "QPn349z25fnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(output_list[0].detach().numpy())\n",
        "\n",
        "# save the dataframe to a csv file\n",
        "df.to_csv('outputs.csv', index=False)\n"
      ],
      "metadata": {
        "id": "YpSALyzsa5xU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_files, labels):\n",
        "        self.audio_files = audio_files\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.audio_files[idx]\n",
        "        label = self.labels[idx]\n",
        "        return audio, label\n"
      ],
      "metadata": {
        "id": "VGG2rXlcJLtQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = arr\n",
        "labels = [1,0,0,0,0,0,1,1,1,1]\n",
        "dataset = AudioDataset(audio_files, labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mLZ9o3lb4cp",
        "outputId": "9209bea5-9f11-49c6-ebe0-b11c82249559"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define your classifier\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Train the classifier on your dataset\n",
        "for epoch in range(5):\n",
        "    for i, (audio_logits, labels) in enumerate(dataloader):\n",
        "        # Convert the logits and labels to Pytorch tensors\n",
        "        audio_logits = torch.from_numpy(audio_logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = Classifier(audio_logits)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "91aQrJZ_96eT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}